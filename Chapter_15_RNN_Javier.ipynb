{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 15 RNN Javier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKZuFzLIkMOi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solucionar el tema de los unstable gradients:"
      ],
      "metadata": {
        "id": "Li_6GLhRxpkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicar Layer Normalization a una SimpleRNNCell:\n",
        "\n",
        "SimpleRNNCell: This class processes one step within the whole time sequence input, whereas tf.keras.layer.SimpleRNN processes the whole sequence."
      ],
      "metadata": {
        "id": "MFlldXvokUUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # en una SimpleRNNCell, both state_size and output_size son el numero de units\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        # no activation porque queremos hacer LN entre medias\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    # initial state para la primera llamada (h_init)\n",
        "    def get_initial_state(self, inputs = None, batch_size = None, dtype = None):\n",
        "        if inputs is not None:\n",
        "            batch_size = tf.shape(inputs)[0] # (batch, ...)\n",
        "            dtype = inputs.dtype\n",
        "        return [tf.zeros([batch_size, self.state_size], dtype = dtype)]\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "      # outputs == new_states, porque es una SimpleRNNCell\n",
        "      # orden: RNNCell -> LN -> Activation\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        return norm_outputs, [norm_outputs] # output, hidden_state\n",
        "        "
      ],
      "metadata": {
        "id": "EPqmxsiikUEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    # hacemos Keras RNN layer a partir de las Cells que hemos definido\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
        "                     input_shape=[None, 1]), # None = timesteps, 1 = features univariate\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
        "    # return_sequences = True en todos los casos anteriores porque es seq-to-seq\n",
        "    # TimeDistributed para que se aplique la Dense(10) a cada time step\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "metadata": {
        "id": "Yt9CmUe6s_fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# para la evaluacion queremos tener en cuenta solo el mse del ultimo time_step\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    # batch_size, time_steps, features\n",
        "    # [:, -1] es todo el batch, ultimo time_step (la tercera dimension se ignora y se coge todo entiendo)\n",
        "    # otro ejemplo tonto, [0] en este caso seria primer batch, todo de las demas\n",
        "    # como time_steps es el segundo, pues -1 en el segundo y ya esta\n",
        "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])"
      ],
      "metadata": {
        "id": "P3WtweawuMFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"mse\", # en general pq es la loss\n",
        "              optimizer = \"adam\",\n",
        "              metrics = [last_time_step_mse], # para eval, mse solo last time step\n",
        ")\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs = 20, validation_data = (X_valid, Y_valid))"
      ],
      "metadata": {
        "id": "-f-GpZy4w012"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout (inputs) y Recurrent Dropout (hidden states):"
      ],
      "metadata": {
        "id": "faSaJiv8xuax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "eso lo implementa Keras en todas las RNN (menos RNN) y en todas las RNNCells"
      ],
      "metadata": {
        "id": "U_AuHQwry4Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GUylioS9y5Lv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}