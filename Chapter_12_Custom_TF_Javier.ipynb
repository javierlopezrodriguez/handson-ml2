{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_12_Custom_TF_Javier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LXwdQWKU69ek"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.4 is required in this notebook\n",
        "# Earlier 2.x versions will mostly work the same, but with a few bugs\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.4\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom loss function:**"
      ],
      "metadata": {
        "id": "SVvVsDlC7T0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin hiperparams"
      ],
      "metadata": {
        "id": "Mt2lbLhG84tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# como función (usar TF dentro en todo):\n",
        "def huber_fn(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = 1 * tf.abs(error) - 1**2 / 2\n",
        "    # donde sea error chiquito, devuelve squared_loss, si no, devuelve linear_loss\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "# model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
        "\n",
        "# model.save(\"my_model_with_a_custom_loss.h5\")\n",
        "\n",
        "# model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
        "#                                 custom_objects={\"huber_fn\": huber_fn})"
      ],
      "metadata": {
        "id": "cWIkqVgY7PDQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con hiperparams (dos opciones, función normal (peor), subclass Loss (mejor))"
      ],
      "metadata": {
        "id": "n70hqOWV86AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# como función devuelta por otra función (threshold personalizable):\n",
        "def create_huber(threshold = 1.0):\n",
        "    # la misma funcion pero con threshold en vez del valor hard-coded 1\n",
        "    def huber_fn(y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
        "        # donde sea error chiquito, devuelve squared_loss, si no, devuelve linear_loss\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    return huber_fn\n",
        "\n",
        "# model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
        "\n",
        "# model.save(\"my_model_with_a_custom_loss_threshold_2.h5\")\n",
        "\n",
        "# model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
        "#                                 custom_objects={\"huber_fn\": create_huber(2.0)})"
      ],
      "metadata": {
        "id": "KScSoSjb8Rx8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# como subclass de keras.losses.Loss:\n",
        "class HuberLoss(keras.losses.Loss):\n",
        "    # init\n",
        "    def __init__(self, threshold = 1.0, **kwargs):\n",
        "        self.threshold = threshold\n",
        "        super().__init__(**kwargs)\n",
        "    # call, lo mismo que la funcion, pero usando self.threshold OJO!!!\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "        # donde sea error chiquito, devuelve squared_loss, si no, devuelve linear_loss\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "    # para poder guardar los hiperparametros\n",
        "    def get_config(self): # para poder guardar los hiperparametros\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n",
        "\n",
        "\n",
        "# model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
        "\n",
        "# model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
        "\n",
        "# al definir get_config se puede guardar el threshold automaticamente, y asi despues:\n",
        "\n",
        "# model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
        "#                                 custom_objects={\"HuberLoss\": HuberLoss})\n",
        "\n",
        "# eso usa el config (guardado con el modelo en el h5) y crea la loss correspondiente\n",
        "\n",
        "# model.loss.threshold --> 2.0"
      ],
      "metadata": {
        "id": "TAf1Riw582S1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom metrics:**"
      ],
      "metadata": {
        "id": "TaCPdM7lBRHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming metric (como la precision, que no sea simplemente la average de la metric en cada batch sino que tenga en cuenta toda la info necesaria del epoch para calcularla a medida que se avanza):"
      ],
      "metadata": {
        "id": "_Tc1enctBTmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subclass de keras.metrics.Mean, que es mejor que keras.metrics.Metrics para estas cosas\n",
        "class HuberMetric(keras.metrics.Mean):\n",
        "\n",
        "    def __init__(self, threshold = 1.0, name = \"HuberMetric\", dtype = None):\n",
        "        self.threshold = threshold\n",
        "        self.huber_fn = create_huber(threshold)\n",
        "        super().__init__(name = name, dtype = dtype)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
        "        metric = self.huber_fn(y_true, y_pred)\n",
        "        # entiendo que esto es lo mismo que hacer super().update_state(...)\n",
        "        # se encarga la clase Mean de hacer el update state y tener en cuenta las sample_weights\n",
        "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n",
        "\n",
        "# model.compile(loss=keras.losses.Huber(2.0), optimizer=\"nadam\", weighted_metrics=[HuberMetric(2.0)])\n",
        "\n",
        "# sample_weight = np.random.rand(len(y_train))\n",
        "# history = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32),\n",
        "#                     epochs=2, sample_weight=sample_weight)\n",
        "\n",
        "# model.save(\"my_model_with_a_custom_metric_v2.h5\")\n",
        "\n",
        "# model = keras.models.load_model(\"my_model_with_a_custom_metric_v2.h5\",\n",
        "#                                 custom_objects={\"HuberMetric\": HuberMetric})"
      ],
      "metadata": {
        "id": "omdPBoyZBS5R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio custom layers:**"
      ],
      "metadata": {
        "id": "cCzTOaZDHTR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLayerNormalization(keras.layers.Layer):\n",
        "\n",
        "# epsilon es un hiperparametro\n",
        "    def __init__(self, epsilon = 0.001, **kwargs):\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        self.alpha = self.add_weight(name = \"alpha\",\n",
        "                                     shape = batch_input_shape[-1:],\n",
        "                                     dtype = tf.float32,\n",
        "                                     initializer = \"ones\")\n",
        "        \n",
        "        self.beta = self.add_weight(name = \"beta\",\n",
        "                                    shape = batch_input_shape[-1:],\n",
        "                                    dtype = tf.float32,\n",
        "                                    initializer = \"zeros\")\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, X):\n",
        "        mean, variance = tf.nn.moments(X, axes = -1, keepdims = True)\n",
        "        # OJO: sqrt(0) no tiene derivada, asi que mejor meter el self.epsilon DENTRO\n",
        "        std = tf.sqrt(variance + self.epsilon)\n",
        "        # ojo, usa siempre los self\n",
        "        result = self.alpha*(X - mean)/std + self.beta\n",
        "        return result\n",
        "      \n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return batch_input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"epsilon\": self.epsilon}"
      ],
      "metadata": {
        "id": "S9wcjAsGHVW8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "f9bvLmieMGPX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_layer_norm = CustomLayerNormalization()\n",
        "\n",
        "keras_layer_norm = keras.layers.LayerNormalization()\n",
        "\n",
        "X = X_train.astype(np.float32)\n",
        "\n",
        "tf.reduce_mean(keras.losses.mean_squared_error(keras_layer_norm(X), custom_layer_norm(X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5ZeXbBIMLGn",
        "outputId": "d4787c52-768f-4179-a890-8b776f776cbb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=1.0791172e-14>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}